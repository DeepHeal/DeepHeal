{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepHeal Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the `DeepHeal` package programmatically to learn low-dimensional representations of drug-induced proteomic changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DeepHeal if running in Google Colab\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install git+https://github.com/DeepHeal/DeepHeal.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepheal.deepheal import DeepHeal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Synthetic Data\n",
    "\n",
    "For this tutorial, we will create a synthetic dataset simulating proteomics log2 fold-changes. We will simulate 100 samples belonging to 3 different drug classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_samples = 100\n",
    "n_features = 500\n",
    "n_classes = 3\n",
    "\n",
    "# Generate random features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Add some structure based on class\n",
    "y = np.random.randint(0, n_classes, n_samples)\n",
    "for i in range(n_samples):\n",
    "    X[i, :] += y[i] * 0.5  # Shift mean based on class\n",
    "\n",
    "# Create Sample IDs\n",
    "sample_ids = [f\"Sample_{i}\" for i in range(n_samples)]\n",
    "drug_classes = [f\"Class_{label}\" for label in y]\n",
    "\n",
    "# Create Input DataFrame (Features)\n",
    "input_df = pd.DataFrame(X, columns=[f\"Prot_{i}\" for i in range(n_features)])\n",
    "input_df.insert(0, \"Sample_ID\", sample_ids)\n",
    "\n",
    "# Create Meta DataFrame (Labels)\n",
    "meta_df = pd.DataFrame({\n",
    "    \"Sample_ID\": sample_ids,\n",
    "    \"Drug_Class\": drug_classes\n",
    "})\n",
    "\n",
    "print(\"Input Data:\")\n",
    "print(input_df.head())\n",
    "print(\"\\nMeta Data:\")\n",
    "print(meta_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize and Train DeepHeal\n",
    "\n",
    "We initialize the model with desired hyperparameters. Here we use `no_batch=True` (implied by not providing a domain key or setting it to None) as we are doing simple dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = DeepHeal(\n",
    "    save_dir=\"tutorial_output\",\n",
    "    latent_dim=16,\n",
    "    n_epochs=10,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    verbose=True,\n",
    "    use_importance_mask=True,\n",
    "    domain_key=None, # No batch correction\n",
    ")\n",
    "\n",
    "# Prepare data (drop ID column from features)\n",
    "features = input_df.drop(columns=[\"Sample_ID\"])\n",
    "\n",
    "# Set data\n",
    "model.set_data(features, meta_data=meta_df)\n",
    "\n",
    "# Train\n",
    "model.train(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings\n",
    "\n",
    "After training, we can extract the low-dimensional embeddings for our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.predict(features)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Create a DataFrame for results\n",
    "embedding_df = pd.DataFrame(embeddings, columns=[f\"z{i+1}\" for i in range(embeddings.shape[1])])\n",
    "embedding_df[\"Drug_Class\"] = meta_df[\"Drug_Class\"]\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "\n",
    "Let's visualize the learned embeddings using PCA to see if the classes separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cls in np.unique(drug_classes):\n",
    "    mask = np.array(drug_classes) == cls\n",
    "    plt.scatter(pca_result[mask, 0], pca_result[mask, 1], label=cls, alpha=0.7)\n",
    "\n",
    "plt.title(\"PCA of DeepHeal Embeddings\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Since we enabled `use_importance_mask=True`, we can analyze which features (proteins) were most important for the learned representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepheal.utils import compute_feature_importance, prepare_ranked_list\n",
    "\n",
    "# Ensure model is in eval mode and prepare input tensor\n",
    "model.model.eval()\n",
    "input_tensor = torch.tensor(features.values, dtype=torch.float32).to(model.config.device)\n",
    "\n",
    "# Compute importance for the last layer of the encoder\n",
    "# This gives us the contribution of input features to the latent representation\n",
    "layer_idx = len(model.model.encoder) - 1\n",
    "importance_matrix = compute_feature_importance(model.model, input_tensor, layer_idx)\n",
    "\n",
    "print(f\"Importance Matrix Shape: {importance_matrix.shape}\")\n",
    "\n",
    "# Prepare ranked lists of features\n",
    "ranked_lists = prepare_ranked_list(importance_matrix, features.columns, expression_df=features)\n",
    "\n",
    "# Display top 10 important features for the first neuron in the latent space\n",
    "neuron_name = 'Neuron 0'\n",
    "if neuron_name in ranked_lists:\n",
    "    print(f\"\\nTop features for {neuron_name}:\")\n",
    "    print(ranked_lists[neuron_name].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Other Methods\n",
    "\n",
    "We compare DeepHeal with standard dimensionality reduction methods: PCA, NMF, and ICA.\n",
    "We use the **Silhouette Score** to quantify how well the drug classes are separated in the low-dimensional space, and visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, NMF, FastICA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Prepare Data\n",
    "X = features.values\n",
    "y_labels = meta_df[\"Drug_Class\"]\n",
    "\n",
    "# NMF requires non-negative data\n",
    "X_pos = X - X.min()\n",
    "\n",
    "# 1. Calculate Silhouette Scores (Quantitative)\n",
    "# For fair comparison, we use the same latent dimension (16) for all methods\n",
    "latent_dim = 16\n",
    "\n",
    "# DeepHeal (already computed)\n",
    "score_dh = silhouette_score(embeddings, y_labels)\n",
    "\n",
    "# PCA\n",
    "pca_16 = PCA(n_components=latent_dim).fit_transform(X)\n",
    "score_pca = silhouette_score(pca_16, y_labels)\n",
    "\n",
    "# NMF\n",
    "nmf_16 = NMF(n_components=latent_dim, init='random', random_state=42, max_iter=500).fit_transform(X_pos)\n",
    "score_nmf = silhouette_score(nmf_16, y_labels)\n",
    "\n",
    "# ICA\n",
    "ica_16 = FastICA(n_components=latent_dim, random_state=42, max_iter=500).fit_transform(X)\n",
    "score_ica = silhouette_score(ica_16, y_labels)\n",
    "\n",
    "print(f\"Silhouette Score (DeepHeal): {score_dh:.4f}\")\n",
    "print(f\"Silhouette Score (PCA):      {score_pca:.4f}\")\n",
    "print(f\"Silhouette Score (NMF):      {score_nmf:.4f}\")\n",
    "print(f\"Silhouette Score (ICA):      {score_ica:.4f}\")\n",
    "\n",
    "# 2. Visualization (2D)\n",
    "# We project the embeddings to 2D for visualization.\n",
    "# For DeepHeal, we use PCA on the 16D embeddings (consistent with Section 4).\n",
    "# For baselines, we compute 2D projections directly from data.\n",
    "\n",
    "pca_vis = PCA(n_components=2)\n",
    "z_dh_2d = pca_vis.fit_transform(embeddings)\n",
    "\n",
    "z_pca_2d = PCA(n_components=2).fit_transform(X)\n",
    "z_nmf_2d = NMF(n_components=2, init='random', random_state=42, max_iter=500).fit_transform(X_pos)\n",
    "z_ica_2d = FastICA(n_components=2, random_state=42, max_iter=500).fit_transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "methods = [\n",
    "    (\"DeepHeal\", z_dh_2d, score_dh),\n",
    "    (\"PCA\", z_pca_2d, score_pca),\n",
    "    (\"NMF\", z_nmf_2d, score_nmf),\n",
    "    (\"ICA\", z_ica_2d, score_ica)\n",
    "]\n",
    "\n",
    "unique_classes = np.unique(y_labels)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_classes)))\n",
    "\n",
    "for ax, (name, z_2d, score) in zip(axes.flatten(), methods):\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        mask = (y_labels == cls)\n",
    "        ax.scatter(z_2d[mask, 0], z_2d[mask, 1], label=cls, alpha=0.7, color=colors[i])\n",
    "    ax.set_title(f\"{name} (Silhouette: {score:.3f})\")\n",
    "    ax.set_xlabel(\"Dim 1\")\n",
    "    ax.set_ylabel(\"Dim 2\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}